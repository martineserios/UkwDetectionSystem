{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtxpJfUMloSb",
        "outputId": "cabde29d-1705-455b-c53f-a23a2c35f79b"
      },
      "outputs": [],
      "source": [
        "# For larger models with more than 2gb memory\n",
        "# !git clone https://github.com/zldrobit/yolov5.git  # clone\n",
        "# %cd yolov5\n",
        "# !git checkout fix-exceeds-2gb\n",
        "\n",
        "# !git clone https://github.com/ultralytics/yolov5\n",
        "# %cd yolov5\n",
        "# %pip install -qr requirements.txt  # install\n",
        "\n",
        "\n",
        "from yolov5 import utils\n",
        "# display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlAqR7PJmvTL"
      },
      "source": [
        "# Convert to ONNX\n",
        "Select the model version and input size. Default: YOLOV6s (640x480)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import shutil\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "# Libraries for pre and post processsing\n",
        "# from ultralytics.data.augment import LetterBox\n",
        "# from ultralytics.engine.results import Results\n",
        "# from ultralytics.utils import ops\n",
        "# from ultralytics.utils.plotting import Annotator, colors\n",
        "\n",
        "# import onnx_runtime related package\n",
        "import onnxruntime as rt\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class ModelHandler(object):\n",
        "    \"\"\"\n",
        "    A YOLOV5 Model handler implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.initialized = False\n",
        "\n",
        "        # Parameters for inference\n",
        "        self.model = None\n",
        "\n",
        "        # Parameters for pre-processing\n",
        "        self.imgsz = 640 # default value for this usecase. \n",
        "        self.stride = 32 # default value for this usecase( differs based on the model selected )\n",
        "        \n",
        "        # Parameters for post-processing\n",
        "        self.conf = 0.25\n",
        "        self.iou = 0.45\n",
        "        self.max_det = 300\n",
        "        self.classes = None\n",
        "        self.agnostic = False\n",
        "        self.labels = {0: 'somke', 1: 'fire'}\n",
        "        \n",
        "        self.path = '/home/raw-data/'\n",
        "        \n",
        "        # print(os.listdir(\"/opt/ml/model/\"))\n",
        "\n",
        "    def initialize(self, context):\n",
        "        # os.path.join(model_dir,'model_onnx.onnx')\n",
        "        self.initialized = True\n",
        "\n",
        "        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path='/home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.pt')\n",
        "        \n",
        "    def preprocess(self, request):\n",
        "        image_url = request['body'].decode()\n",
        "\n",
        "        ## Set up the image URL and filename\n",
        "        self.path = self.path+image_url.split(\"/\")[-1] if self.path == '/home/raw-data/' else '/home/raw-data/'+image_url.split(\"/\")[-1]\n",
        "        # self.path = self.path+self.filename if self.path=='' else\n",
        "\n",
        "        # Open the url image, set stream to True, this will return the stream content.\n",
        "        r = requests.get(image_url, stream = True)\n",
        "\n",
        "        # Check if the image was retrieved successfully\n",
        "        if r.status_code == 200:\n",
        "            # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
        "            r.raw.decode_content = True\n",
        "\n",
        "            # Open a local file with wb ( write binary ) permission.\n",
        "            with open(self.path,'wb') as f:\n",
        "                shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "            print('Image sucessfully downloaded: ',self.path)\n",
        "        else:\n",
        "            print('Image couldn\\'t be retreived')\n",
        "            return \n",
        "        \n",
        "        image_abs_path = os.path.abspath(self.path)\n",
        "        if os.path.isfile(image_abs_path) and image_abs_path.split('.')[-1].lower() in ['jpg', 'jpeg', 'png']:\n",
        "                      \n",
        "            # Load Image\n",
        "            img0 = cv2.imread(\"/home/model-server/vlcsnap.png\")\n",
        "\n",
        "            image = cv2.resize(img0, (640, 640))\n",
        "            image_bchw = np.transpose(np.expand_dims(image, 0), (0, 3, 1, 2))\n",
        "            return img0, image_bchw\n",
        "        else:\n",
        "            print(\"Invalid image format.\")\n",
        "            return\n",
        "\n",
        "    def inference(self, model_input):\n",
        "        print(\"Performing PyTorch prediction\")\n",
        "        start_time = datetime.now()\n",
        "        prediction = self.model([model_input], size=640) # batch of images\n",
        "        end_time = datetime.now()\n",
        "\n",
        "        return prediction, (end_time - start_time).total_seconds()\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        if inference_output is not None:\n",
        "            prediction = inference_output[0]\n",
        "            inference_time = inference_output[1]\n",
        "\n",
        "            # Bytestring option\n",
        "            serialized = pickle.dumps(prediction)\n",
        "            # deserialized_a = pickle.loads(serialized)\n",
        "\n",
        "            # JSON option\n",
        "            # latin-1 maps byte n to unicode code point n\n",
        "            serialized_as_json = json.dumps(pickle.dumps(prediction).decode('latin-1'))\n",
        "            \n",
        "            print(prediction)\n",
        "            # print(serialized_as_json)\n",
        "\n",
        "            # return [f\"inference_time: {inference_time}s\\nInference_summary: {log_string}\\nraw_output:\\n{raw_output}\"]\n",
        "            return serialized_as_json #\"]#{log_string}\\nraw_output:\\n{raw_output}\"]\n",
        "\n",
        "    def handle(self, data, context):\n",
        "        preprocessed_data = self.preprocess(data)\n",
        "        if preprocessed_data:\n",
        "            model_input = preprocessed_data\n",
        "            inference_output = self.inference(model_input)\n",
        "        return self.postprocess(inference_output)\n",
        "\n",
        "_service = ModelHandler()\n",
        "\n",
        "def handle(data, context):\n",
        "    if not _service.initialized:\n",
        "        _service.initialize(context)\n",
        "    \n",
        "    if data is None:\n",
        "        return None\n",
        "\n",
        "    return _service.handle(data, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "handle()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "001B2PzUlyIR",
        "outputId": "1b73b5f6-d663-462d-cb17-55742afb90b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '#yolov5'\n",
            "/home/martin/Projects/ongoing/ukw/model/yolov5/yolov5\n",
            "benchmarks.py\t detect.py   __pycache__       runs\t       val.py\n",
            "CITATION.cff\t export.py   pyproject.toml    segment\t       yolov5s.pt\n",
            "classify\t hubconf.py  README.md\t       train.py\n",
            "CONTRIBUTING.md  LICENSE     README.zh-CN.md   tutorial.ipynb\n",
            "data\t\t models      requirements.txt  utils\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/martin/Projects/ongoing/ukw/model/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
            "  bkms = self.shell.db.get('bookmarks', {})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['/home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=True, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
            "YOLOv5 🚀 v7.0-294-gdb125a20 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.pt with output shape (1, 25200, 7) (13.8 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0...\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx-simplifier>=0.4.1'] not found, attempting AutoUpdate...\n",
            "/home/martin/.pyenv/pyenv.d/exec/pip-rehash/pip: /home/martin/Projects/ongoing/ukw/model/.venv/bin/pip: /home/martin/Projects/ukw/ukw/model/.venv/bin/python: bad interpreter: No such file or directory\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache \"onnx-simplifier>=0.4.1\" ' returned non-zero exit status 126.\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnx-simplifier 0.4.36...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.6s, saved as /home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.onnx (27.2 MB)\n",
            "\n",
            "Export complete (3.8s)\n",
            "Results saved to \u001b[1m/home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre\u001b[0m\n",
            "Detect:          python detect.py --weights /home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.onnx \n",
            "Validate:        python val.py --weights /home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.onnx \n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '/home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.onnx')  \n",
            "Visualize:       https://netron.app\n"
          ]
        }
      ],
      "source": [
        "%cd #yolov5\n",
        "# !pip3 install onnx>=1.10.0\n",
        "model = 'yolov5s6' #@param [\"yolov5n6\", \"yolov5s6\", \"yolov5m6\", \"yolov5l6\", \"yolov5x6\"]\n",
        "input_width = 640 #@param {type:\"slider\", min:64, max:4096, step:64}    \n",
        "input_height = 640 #@param {type:\"slider\", min:64, max:4096, step:64}\n",
        "!ls\n",
        "!python3 export.py --weights /home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.pt --img {input_height} {input_width} --include onnx --simplify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The console stream is logged into /home/martin/sg_logs/console.log\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2024-05-06 13:59:42] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARNING]No module named 'pycocotools'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "I0506 13:59:48.125076 133696912667072 env_sanity_check.py:112] numpy==1.26.4 does not satisfy requirement numpy<=1.23\n",
            "Using cache found in /home/martin/.cache/torch/hub/ultralytics_yolov5_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['pillow>=10.3.0'] not found, attempting AutoUpdate...\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache \"pillow>=10.3.0\" ' returned non-zero exit status 126.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/martin/.pyenv/pyenv.d/exec/pip-rehash/pip: /home/martin/Projects/ongoing/ukw/model/.venv/bin/pip: /home/martin/Projects/ukw/ukw/model/.venv/bin/python: bad interpreter: No such file or directory\n",
            "YOLOv5 🚀 2024-5-4 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "\n",
            "Loading /home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.onnx for ONNX Runtime inference...\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>confidence</th>\n",
              "      <th>class</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>413.728699</td>\n",
              "      <td>303.006348</td>\n",
              "      <td>457.353577</td>\n",
              "      <td>324.346130</td>\n",
              "      <td>0.745950</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>520.301025</td>\n",
              "      <td>241.078064</td>\n",
              "      <td>581.076050</td>\n",
              "      <td>312.585632</td>\n",
              "      <td>0.625424</td>\n",
              "      <td>1</td>\n",
              "      <td>smoke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>375.935272</td>\n",
              "      <td>214.437576</td>\n",
              "      <td>399.287506</td>\n",
              "      <td>240.502853</td>\n",
              "      <td>0.619318</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59.458191</td>\n",
              "      <td>353.438629</td>\n",
              "      <td>104.690796</td>\n",
              "      <td>406.029510</td>\n",
              "      <td>0.520865</td>\n",
              "      <td>1</td>\n",
              "      <td>smoke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>291.413544</td>\n",
              "      <td>281.091492</td>\n",
              "      <td>318.496124</td>\n",
              "      <td>301.168823</td>\n",
              "      <td>0.519207</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>414.060730</td>\n",
              "      <td>241.252884</td>\n",
              "      <td>434.034241</td>\n",
              "      <td>261.348602</td>\n",
              "      <td>0.413312</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>123.702484</td>\n",
              "      <td>325.788879</td>\n",
              "      <td>269.906891</td>\n",
              "      <td>383.386658</td>\n",
              "      <td>0.394778</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>227.961548</td>\n",
              "      <td>54.212837</td>\n",
              "      <td>249.512054</td>\n",
              "      <td>79.270470</td>\n",
              "      <td>0.371741</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>357.098969</td>\n",
              "      <td>252.612656</td>\n",
              "      <td>400.373505</td>\n",
              "      <td>299.891907</td>\n",
              "      <td>0.369598</td>\n",
              "      <td>1</td>\n",
              "      <td>smoke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>163.004089</td>\n",
              "      <td>337.698151</td>\n",
              "      <td>256.178314</td>\n",
              "      <td>376.231171</td>\n",
              "      <td>0.344392</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>107.903183</td>\n",
              "      <td>262.205017</td>\n",
              "      <td>198.418777</td>\n",
              "      <td>336.339111</td>\n",
              "      <td>0.336605</td>\n",
              "      <td>1</td>\n",
              "      <td>smoke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>272.760071</td>\n",
              "      <td>306.216064</td>\n",
              "      <td>288.313660</td>\n",
              "      <td>323.977905</td>\n",
              "      <td>0.332873</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>133.164459</td>\n",
              "      <td>332.381287</td>\n",
              "      <td>203.695831</td>\n",
              "      <td>387.164612</td>\n",
              "      <td>0.332230</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>543.947021</td>\n",
              "      <td>88.498016</td>\n",
              "      <td>638.027344</td>\n",
              "      <td>178.096252</td>\n",
              "      <td>0.319848</td>\n",
              "      <td>1</td>\n",
              "      <td>smoke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>205.513687</td>\n",
              "      <td>345.724609</td>\n",
              "      <td>247.366074</td>\n",
              "      <td>370.867432</td>\n",
              "      <td>0.312914</td>\n",
              "      <td>0</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          xmin        ymin        xmax        ymax  confidence  class   name\n",
              "0   413.728699  303.006348  457.353577  324.346130    0.745950      0   fire\n",
              "1   520.301025  241.078064  581.076050  312.585632    0.625424      1  smoke\n",
              "2   375.935272  214.437576  399.287506  240.502853    0.619318      0   fire\n",
              "3    59.458191  353.438629  104.690796  406.029510    0.520865      1  smoke\n",
              "4   291.413544  281.091492  318.496124  301.168823    0.519207      0   fire\n",
              "5   414.060730  241.252884  434.034241  261.348602    0.413312      0   fire\n",
              "6   123.702484  325.788879  269.906891  383.386658    0.394778      0   fire\n",
              "7   227.961548   54.212837  249.512054   79.270470    0.371741      0   fire\n",
              "8   357.098969  252.612656  400.373505  299.891907    0.369598      1  smoke\n",
              "9   163.004089  337.698151  256.178314  376.231171    0.344392      0   fire\n",
              "10  107.903183  262.205017  198.418777  336.339111    0.336605      1  smoke\n",
              "11  272.760071  306.216064  288.313660  323.977905    0.332873      0   fire\n",
              "12  133.164459  332.381287  203.695831  387.164612    0.332230      0   fire\n",
              "13  543.947021   88.498016  638.027344  178.096252    0.319848      1  smoke\n",
              "14  205.513687  345.724609  247.366074  370.867432    0.312914      0   fire"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 0.3468995900002483 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1: 640x640 10 fires, 5 smokes\n",
            "Speed: 5.3ms pre-process, 321.1ms inference, 18.1ms NMS per image at shape (1, 3, 640, 640)\n",
            "Saved 1 image to \u001b[1m/home/martin/Desktop3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "import time\n",
        "from super_gradients.training.utils.media.image import load_image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/home/martin/Projects/ongoing/ukw/model/yolov5_freeze_ds_fire_3_pre/best.onnx')\n",
        "# # Images\n",
        "# for f in 'zidane.jpg', 'bus.jpg':\n",
        "#     torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)  # download 2 images\n",
        "# im1 = Image.open('zidane.jpg')  # PIL image\n",
        "# im2 = cv2.imread('bus.jpg')[..., ::-1]  # OpenCV image (BGR to RGB)\n",
        "\n",
        "# Inference\n",
        "# Load and preprocess image for testing\n",
        "image = np.asarray(Image.open(\"/home/martin/Desktop/wildfire.jpg\"))\n",
        "image = cv2.resize(image, (640, 640))\n",
        "image_bchw = np.transpose(np.expand_dims(image, 0), (0, 3, 1, 2))\n",
        "\n",
        "    \n",
        "\n",
        "start = time.perf_counter()\n",
        "results = model([image], size=640) # batch of images\n",
        "end = time.perf_counter()\n",
        "print(f\"Time: {end-start} s\")\n",
        "\n",
        "# Results\n",
        "results.print()  \n",
        "results.save(save_dir='/home/martin/Desktop')  # or .show()\n",
        "\n",
        "results.xyxy[0]  # im1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # im1 predictions (pandas)\n",
        "#      xmin    ymin    xmax   ymax  confidence  class    name\n",
        "# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n",
        "# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n",
        "# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n",
        "# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method DataFrame.to_dict of           xmin        ymin        xmax        ymax  confidence  class   name\n",
              "0   413.728699  303.006378  457.353577  324.346100    0.745950      0   fire\n",
              "1   520.301086  241.078064  581.076111  312.585632    0.625423      1  smoke\n",
              "2   375.935242  214.437576  399.287537  240.502853    0.619319      0   fire\n",
              "3    59.458199  353.438629  104.690788  406.029510    0.520866      1  smoke\n",
              "4   291.413544  281.091492  318.496124  301.168823    0.519207      0   fire\n",
              "5   414.060699  241.252884  434.034210  261.348572    0.413311      0   fire\n",
              "6   123.702484  325.788879  269.906891  383.386658    0.394778      0   fire\n",
              "7   227.961563   54.212845  249.512039   79.270462    0.371742      0   fire\n",
              "8   357.098969  252.612640  400.373505  299.891937    0.369598      1  smoke\n",
              "9   163.004089  337.698151  256.178345  376.231171    0.344392      0   fire\n",
              "10  107.903175  262.204987  198.418793  336.339081    0.336605      1  smoke\n",
              "11  272.760071  306.216064  288.313660  323.977905    0.332873      0   fire\n",
              "12  133.164474  332.381287  203.695816  387.164612    0.332230      0   fire\n",
              "13  543.947021   88.498016  638.027344  178.096222    0.319848      1  smoke\n",
              "14  205.513687  345.724609  247.366074  370.867432    0.312914      0   fire>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results.pandas().xyxy[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected object with class_id=1, confidence=0.35458868741989136, x_min=0.5154963731765747, y_min=0.41392049193382263, x_max=0.5646795034408569, y_max=0.4997726380825043\n",
            "Detected object with class_id=1, confidence=0.340872198343277, x_min=0.5772157907485962, y_min=0.46482688188552856, x_max=0.6342465281486511, y_max=0.5345455408096313\n",
            "Detected object with class_id=1, confidence=0.2519384026527405, x_min=0.5353363752365112, y_min=0.43407759070396423, x_max=0.6365019679069519, y_max=0.5308430194854736\n"
          ]
        }
      ],
      "source": [
        "flat_predictions = results.xyxyn\n",
        "for (x_min, y_min, x_max, y_max, confidence, class_id) in flat_predictions[0]:\n",
        "    class_id = int(class_id)\n",
        "    print(f\"Detected object with class_id={class_id}, confidence={confidence}, x_min={x_min}, y_min={y_min}, x_max={x_max}, y_max={y_max}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[0.51550, 0.41392, 0.56468, 0.49977, 0.35459, 1.00000],\n",
              "         [0.57722, 0.46483, 0.63425, 0.53455, 0.34087, 1.00000],\n",
              "         [0.53534, 0.43408, 0.63650, 0.53084, 0.25194, 1.00000]])]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[i[0] for i in flat_predictions]\n",
        "\n",
        "flat_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from super_gradients.training.datasets.datasets_conf import COCO_DETECTION_CLASSES_LIST\n",
        "from super_gradients.training.utils.detection_utils import DetectionVisualization\n",
        "\n",
        "def show_predictions_from_flat_format(image, predictions):\n",
        "    [flat_predictions] = predictions\n",
        "\n",
        "    image = image.copy()\n",
        "    class_names = ['smoke', 'fire'] #COCO_DETECTION_CLASSES_LIST\n",
        "    color_mapping = DetectionVisualization._generate_color_mapping(len(class_names))\n",
        "\n",
        "    for (x1, y1, x2, y2, class_score, class_index) in flat_predictions:\n",
        "        class_index = int(class_index)\n",
        "        image = DetectionVisualization.draw_box_title(\n",
        "                    image_np=image,\n",
        "                    x1=int(x1),\n",
        "                    y1=int(y1),\n",
        "                    x2=int(x2),\n",
        "                    y2=int(y2),\n",
        "                    class_id=class_index,\n",
        "                    class_names=class_names,\n",
        "                    color_mapping=color_mapping,\n",
        "                    box_thickness=1,\n",
        "                    pred_conf=class_score,\n",
        "                )\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "type Tensor doesn't define __round__ method",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(image, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m))\n\u001b[1;32m      5\u001b[0m image_bchw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mexpand_dims(image, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mshow_predictions_from_flat_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_predictions\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[31], line 14\u001b[0m, in \u001b[0;36mshow_predictions_from_flat_format\u001b[0;34m(image, predictions)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x1, y1, x2, y2, class_score, class_index) \u001b[38;5;129;01min\u001b[39;00m flat_predictions:\n\u001b[1;32m     13\u001b[0m     class_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(class_index)\n\u001b[0;32m---> 14\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mDetectionVisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_box_title\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mx1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43my1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43my2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mclass_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbox_thickness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpred_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
            "File \u001b[0;32m~/Projects/ongoing/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/utils/detection_utils.py:457\u001b[0m, in \u001b[0;36mDetectionVisualization.draw_box_title\u001b[0;34m(color_mapping, class_names, box_thickness, image_np, x1, y1, x2, y2, class_id, pred_conf, bbox_prefix)\u001b[0m\n\u001b[1;32m    455\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m image_np \u001b[38;5;241m=\u001b[39m draw_bbox(image\u001b[38;5;241m=\u001b[39mimage_np, title\u001b[38;5;241m=\u001b[39mtitle, x1\u001b[38;5;241m=\u001b[39mx1, y1\u001b[38;5;241m=\u001b[39my1, x2\u001b[38;5;241m=\u001b[39mx2, y2\u001b[38;5;241m=\u001b[39my2, box_thickness\u001b[38;5;241m=\u001b[39mbox_thickness, color\u001b[38;5;241m=\u001b[39mcolor)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_np\n",
            "\u001b[0;31mTypeError\u001b[0m: type Tensor doesn't define __round__ method"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "image = load_image('/home/martin/Desktop/ukw/test_def.jpeg')\n",
        "image = cv2.resize(image, (640, 640))\n",
        "image_bchw = np.transpose(np.expand_dims(image, 0), (0, 3, 1, 2))\n",
        "\n",
        "show_predictions_from_flat_format(image, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "YOLOv5 <class 'models.common.Detections'> instance\n",
              "image 1/1: 640x640 3 smokes\n",
              "Speed: 7.3ms pre-process, 415.9ms inference, 15.9ms NMS per image at shape (1, 3, 640, 640)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = ['smoke', 'fire']\n",
        "colors = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
        "\n",
        "# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n",
        "def yolo2bbox(bboxes):\n",
        "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
        "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
        "    return xmin, ymin, xmax, yma\n",
        "\n",
        "\n",
        "def plot_box(image, bboxes, labels):\n",
        "    # Need the image height and width to denormalize\n",
        "    # the bounding box coordinates\n",
        "    h, w, _ = image.shape\n",
        "    for box_num, box in enumerate(bboxes):\n",
        "        x1, y1, x2, y2 = yolo2bbox(box)\n",
        "        # denormalize the coordinates\n",
        "        xmin = int(x1*w)\n",
        "        ymin = int(y1*h)\n",
        "        xmax = int(x2*w)\n",
        "        ymax = int(y2*h)\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        class_name = class_names[int(labels[box_num])]\n",
        "\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (xmin, ymin), (xmax, ymax),\n",
        "            color=colors[class_names.index(class_name)],\n",
        "            thickness=2\n",
        "        )\n",
        "\n",
        "        font_scale = min(1,max(3,int(w/500)))\n",
        "        font_thickness = min(2, max(10,int(w/50)))\n",
        "\n",
        "        p1, p2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
        "        # Text width and height\n",
        "        tw, th = cv2.getTextSize(\n",
        "            class_name,\n",
        "            0, fontScale=font_scale, thickness=font_thickness\n",
        "        )[0]\n",
        "        p2 = p1[0] + tw, p1[1] + -th - 10\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            p1, p2,\n",
        "            color=colors[class_names.index(class_name)],\n",
        "            thickness=-1,\n",
        "        )\n",
        "        cv2.putText(\n",
        "            image,\n",
        "            class_name,\n",
        "            (xmin+1, ymin-10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            font_scale,\n",
        "            (255, 255, 255),\n",
        "            font_thickness\n",
        "        )\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'float' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxyxy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mplot_box\u001b[0;34m(image, bboxes, labels)\u001b[0m\n\u001b[1;32m     14\u001b[0m h, w, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box_num, box \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bboxes):\n\u001b[0;32m---> 16\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[43myolo2bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# denormalize the coordinates\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     xmin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x1\u001b[38;5;241m*\u001b[39mw)\n",
            "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36myolo2bbox\u001b[0;34m(bboxes)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myolo2bbox\u001b[39m(bboxes):\n\u001b[0;32m----> 6\u001b[0m     xmin, ymin \u001b[38;5;241m=\u001b[39m \u001b[43mbboxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39mbboxes[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, bboxes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mbboxes[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m     xmax, ymax \u001b[38;5;241m=\u001b[39m bboxes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mbboxes[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, bboxes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mbboxes[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xmin, ymin, xmax, yma\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "plot_box(image, np.array(results.xyxy[0][0][:4]).tolist(), class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2024-03-17 16:26:01] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
            "[2024-03-17 16:26:01] WARNING - __init__.py - Failed to import pytorch_quantization\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The console stream is logged into /home/martin/sg_logs/console.log\n",
            "[WARNING]No module named 'pycocotools'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2024-03-17 16:26:02] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
            "[2024-03-17 16:26:02] WARNING - export.py - Failed to import pytorch_quantization\n",
            "[2024-03-17 16:26:02] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "from super_gradients.training import dataloaders\n",
        "from super_gradients.training.dataloaders.dataloaders import coco_detection_yolo_format_train, coco_detection_yolo_format_val\n",
        "from super_gradients.training import models\n",
        "from super_gradients.training import Trainer\n",
        "from super_gradients.training.losses import PPYoloELoss\n",
        "from super_gradients.training.metrics import DetectionMetrics_050\n",
        "from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_params = {\n",
        "    'data_dir':'/home/martin/Projects/ukw/ukw/data/wildfire-2',\n",
        "    'train_images_dir':'train/images',\n",
        "    'train_labels_dir':'train/labels',\n",
        "    'val_images_dir':'valid/images',\n",
        "    'val_labels_dir':'valid/labels',\n",
        "    'test_images_dir':'test/images',\n",
        "    'test_labels_dir':'test/labels',\n",
        "    'classes': ['smoke']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2024-03-17 16:27:17] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n",
            "Indexing dataset annotations: 100%|██████████| 4836/4836 [00:00<00:00, 12735.96it/s]\n",
            "[2024-03-17 16:27:17] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n",
            "Indexing dataset annotations: 100%|██████████| 1379/1379 [00:00<00:00, 13768.54it/s]\n",
            "[2024-03-17 16:27:17] INFO - detection_dataset.py - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.\n",
            "Indexing dataset annotations:   0%|          | 0/692 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Indexing dataset annotations: 100%|██████████| 692/692 [00:00<00:00, 12855.71it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_data = coco_detection_yolo_format_train(\n",
        "    dataset_params={\n",
        "        'data_dir': dataset_params['data_dir'],\n",
        "        'images_dir': dataset_params['train_images_dir'],\n",
        "        'labels_dir': dataset_params['train_labels_dir'],\n",
        "        'classes': dataset_params['classes']\n",
        "    },\n",
        "    dataloader_params={\n",
        "        'batch_size':16,\n",
        "        'num_workers':2\n",
        "    }\n",
        ")\n",
        "\n",
        "val_data = coco_detection_yolo_format_val(\n",
        "    dataset_params={\n",
        "        'data_dir': dataset_params['data_dir'],\n",
        "        'images_dir': dataset_params['val_images_dir'],\n",
        "        'labels_dir': dataset_params['val_labels_dir'],\n",
        "        'classes': dataset_params['classes']\n",
        "    },\n",
        "    dataloader_params={\n",
        "        'batch_size':16,\n",
        "        'num_workers':2\n",
        "    }\n",
        ")\n",
        "\n",
        "test_data = coco_detection_yolo_format_val(\n",
        "    dataset_params={\n",
        "        'data_dir': dataset_params['data_dir'],\n",
        "        'images_dir': dataset_params['test_images_dir'],\n",
        "        'labels_dir': dataset_params['test_labels_dir'],\n",
        "        'classes': dataset_params['classes']\n",
        "    },\n",
        "    dataloader_params={\n",
        "        'batch_size':16,\n",
        "        'num_workers':2\n",
        "    }\n",
        ")\n",
        "\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/martin/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/losses/ppyolo_loss.py:669: DeprecationWarning: A reg_max argument is not needed for PPYoloE loss anymore. It is deprecated since SG 3.6.0 and will be removed in the SG 3.8.0.You can safely omit this argument as it is not used anymore and we infer it automatically from model's outputs\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_params = {\n",
        "    # ENABLING SILENT MODE\n",
        "    'silent_mode': False,\n",
        "    \"average_best_models\":True,\n",
        "    \"warmup_mode\": \"linear_epoch_step\",\n",
        "    \"warmup_initial_lr\": 1e-6,\n",
        "    \"lr_warmup_epochs\": 3,\n",
        "    \"initial_lr\": 5e-4,\n",
        "    \"lr_mode\": \"cosine\",\n",
        "    \"cosine_final_lr_ratio\": 0.1,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"optimizer_params\": {\"weight_decay\": 0.0001},\n",
        "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
        "    \"ema\": True,\n",
        "    \"ema_params\": {\"decay\": 0.9, \"decay_type\": \"threshold\"},\n",
        "    # ONLY TRAINING FOR 10 EPOCHS FOR THIS EXAMPLE NOTEBOOK\n",
        "    \"max_epochs\": 10,\n",
        "    \"mixed_precision\": True,\n",
        "    \"loss\": PPYoloELoss(\n",
        "        use_static_assigner=False,\n",
        "        # NOTE: num_classes needs to be defined here\n",
        "        num_classes=len(dataset_params['classes']),\n",
        "        reg_max=16\n",
        "    ),\n",
        "    \"valid_metrics_list\": [\n",
        "        DetectionMetrics_050(\n",
        "            score_thres=0.1,\n",
        "            top_k_predictions=300,\n",
        "            # NOTE: num_classes needs to be defined here\n",
        "            num_cls=len(dataset_params['classes']),\n",
        "            normalize_targets=True,\n",
        "            post_prediction_callback=PPYoloEPostPredictionCallback(\n",
        "                score_threshold=0.01,\n",
        "                nms_top_k=1000,\n",
        "                max_predictions=300,\n",
        "                nms_threshold=0.7\n",
        "            )\n",
        "        )\n",
        "    ],\n",
        "    \"metric_to_watch\": 'mAP@0.50'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/martin/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2024-3-16 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load(\n",
        "    \"ultralytics/yolov5\", \"custom\", \"best.pt\"\n",
        ")  # load from PyTorch Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = 'checkpoints'\n",
        "\n",
        "trainer = Trainer(experiment_name='yolonas_s', ckpt_root_dir=CHECKPOINT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2024-03-17 17:03:32] INFO - sg_trainer.py - Starting a new run with `run_id=RUN_20240317_170332_950849`\n",
            "[2024-03-17 17:03:32] INFO - sg_trainer.py - Checkpoints directory: checkpoints/yolonas_s/RUN_20240317_170332_950849\n",
            "[2024-03-17 17:03:32] INFO - sg_trainer.py - Using EMA with params {'decay': 0.9, 'decay_type': 'threshold'}\n",
            "/home/martin/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/common/registry/registry.py:72: DeprecationWarning: Object name `linear_epoch_step` is now deprecated. Please replace it with `LinearEpochLRWarmup`.\n",
            "  warnings.warn(f\"Object name `{name}` is now deprecated. Please replace it with `{deprecated_names[name]}`.\", DeprecationWarning)\n",
            "[2024-03-17 17:03:32] INFO - sg_trainer_utils.py - TRAINING PARAMETERS:\n",
            "    - Mode:                         Single GPU\n",
            "    - Number of GPUs:               0          (0 available on the machine)\n",
            "    - Full dataset size:            4836       (len(train_set))\n",
            "    - Batch size per GPU:           16         (batch_size)\n",
            "    - Batch Accumulate:             1          (batch_accumulate)\n",
            "    - Total batch size:             16         (num_gpus * batch_size)\n",
            "    - Effective Batch size:         16         (num_gpus * batch_size * batch_accumulate)\n",
            "    - Iterations per epoch:         302        (len(train_loader))\n",
            "    - Gradient updates per epoch:   302        (len(train_loader) / batch_accumulate)\n",
            "    - Model: AutoShape  (7.01M parameters, 7.01M optimized)\n",
            "    - Learning Rates and Weight Decays:\n",
            "      - default: (7.01M parameters). LR: 0.0005 (7.01M parameters) WD: 0.0, (9.56K parameters), WD: 0.0001, (7.00M parameters)\n",
            "\n",
            "[2024-03-17 17:03:32] INFO - sg_trainer.py - Started training for 10 epochs (0/9)\n",
            "\n",
            "Train epoch 0:   0%|          | 0/302 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The console stream is now moved to checkpoints/yolonas_s/RUN_20240317_170332_950849/console_Mar17_17_03_32.txt\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 6, got 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model: nn.Module,\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/sg_trainer/sg_trainer.py:1530\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, model, training_params, train_loader, valid_loader, test_loaders, additional_configs_to_log)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1524\u001b[0m     device_config\u001b[38;5;241m.\u001b[39mmulti_gpu \u001b[38;5;241m==\u001b[39m MultiGPUMode\u001b[38;5;241m.\u001b[39mDISTRIBUTED_DATA_PARALLEL\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39msampler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1527\u001b[0m ):\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m-> 1530\u001b[0m train_metrics_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# Phase.TRAIN_EPOCH_END\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;66;03m# RUN PHASE CALLBACKS\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m train_metrics_dict \u001b[38;5;241m=\u001b[39m get_metrics_dict(train_metrics_tuple, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metrics, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_logging_items_names)\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/sg_trainer/sg_trainer.py:512\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, context, silent_mode)\u001b[0m\n\u001b[1;32m    509\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(inputs)\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# COMPUTE THE LOSS FOR BACK PROP + EXTRA METRICS COMPUTED DURING THE LOSS FORWARD PASS\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     loss, loss_log_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m context\u001b[38;5;241m.\u001b[39mupdate_context(preds\u001b[38;5;241m=\u001b[39moutputs, loss_log_items\u001b[38;5;241m=\u001b[39mloss_log_items, loss_logging_items_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_logging_items_names)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase_callback_handler\u001b[38;5;241m.\u001b[39mon_train_batch_loss_end(context)\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/sg_trainer/sg_trainer.py:542\u001b[0m, in \u001b[0;36mTrainer._get_losses\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: torch\u001b[38;5;241m.\u001b[39mTensor, targets: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mtuple\u001b[39m]:\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# GET THE OUTPUT OF THE LOSS FUNCTION\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    544\u001b[0m         loss, loss_logging_items \u001b[38;5;241m=\u001b[39m loss\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/losses/ppyolo_loss.py:967\u001b[0m, in \u001b[0;36mPPYoloELoss.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    964\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batched_assignment:\n\u001b[0;32m--> 967\u001b[0m     cls_loss_sum, iou_loss_sum, dfl_loss_sum, assigned_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     cls_loss_sum, iou_loss_sum, dfl_loss_sum, assigned_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_sequential(predictions, targets)\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/losses/ppyolo_loss.py:791\u001b[0m, in \u001b[0;36mPPYoloELoss._forward_batched\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_batched\u001b[39m(\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    779\u001b[0m     predictions: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor],\n\u001b[1;32m    780\u001b[0m     targets: Tensor,\n\u001b[1;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor, Tensor]:\n\u001b[1;32m    782\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    Compute the loss using batched targets-anchors assignment.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m    This is the default way to compute the loss, however it may cause OOM errors when number of targets per image\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m    :return:            Tuple of (classification loss, iou loss, dfl loss, assigned scores sum)\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m     (\n\u001b[1;32m    792\u001b[0m         pred_scores,\n\u001b[1;32m    793\u001b[0m         pred_distri,\n\u001b[1;32m    794\u001b[0m         anchors,\n\u001b[1;32m    795\u001b[0m         anchor_points,\n\u001b[1;32m    796\u001b[0m         num_anchors_list,\n\u001b[1;32m    797\u001b[0m         stride_tensor,\n\u001b[1;32m    798\u001b[0m     ) \u001b[38;5;241m=\u001b[39m predictions\n\u001b[1;32m    800\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_targets_for_batched_assigner(targets, batch_size\u001b[38;5;241m=\u001b[39mpred_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# yolox -> ppyolo\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     anchor_points_s \u001b[38;5;241m=\u001b[39m anchor_points \u001b[38;5;241m/\u001b[39m stride_tensor\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 3)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 0:   0%|          | 0/302 [00:02<?, ?it/s]\n",
            "[2024-03-17 17:03:35] INFO - base_sg_logger.py - [CLEANUP] - Successfully stopped system monitoring process\n"
          ]
        }
      ],
      "source": [
        "# model: nn.Module,\n",
        "\n",
        "trainer.train(model=model,\n",
        "              training_params=train_params,\n",
        "              train_loader=train_data,\n",
        "              valid_loader=val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing:   0%|          | 0/44 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 6)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msuper_gradients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectionMetrics_050\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msuper_gradients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpp_yolo_e\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPYoloEPostPredictionCallback\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_metrics_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDetectionMetrics_050\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_thres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.45\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mtop_k_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mnum_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclasses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mnormalize_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mpost_prediction_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPPYoloEPostPredictionCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                                                                          \u001b[49m\u001b[43mnms_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                                                                          \u001b[49m\u001b[43mmax_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                                                                          \u001b[49m\u001b[43mnms_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/sg_trainer/sg_trainer.py:2114\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, test_loader, loss, silent_mode, test_metrics_list, loss_logging_items_names, metrics_progress_verbose, test_phase_callbacks, use_ema_net)\u001b[0m\n\u001b[1;32m   2111\u001b[0m     context\u001b[38;5;241m.\u001b[39mupdate_context(test_loader\u001b[38;5;241m=\u001b[39mtest_loader)\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase_callback_handler\u001b[38;5;241m.\u001b[39mon_test_loader_start(context)\n\u001b[0;32m-> 2114\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEvaluationType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTEST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_progress_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_progress_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase_callback_handler\u001b[38;5;241m.\u001b[39mon_test_loader_end(context)\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;66;03m# SWITCH BACK BETWEEN NETS SO AN ADDITIONAL TRAINING CAN BE DONE AFTER TEST\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/sg_trainer/sg_trainer.py:2251\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, data_loader, metrics, evaluation_type, epoch, silent_mode, metrics_progress_verbose, dataset_name, max_batches)\u001b[0m\n\u001b[1;32m   2247\u001b[0m context\u001b[38;5;241m.\u001b[39mupdate_context(preds\u001b[38;5;241m=\u001b[39moutput)\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;66;03m# STORE THE loss_items ONLY, THE 1ST RETURNED VALUE IS THE loss FOR BACKPROP DURING TRAINING\u001b[39;00m\n\u001b[0;32m-> 2251\u001b[0m     loss_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m   2252\u001b[0m     context\u001b[38;5;241m.\u001b[39mupdate_context(loss_log_items\u001b[38;5;241m=\u001b[39mloss_tuple)\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;66;03m# TRIGGER PHASE CALLBACKS CORRESPONDING TO THE EVALUATION TYPE\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/sg_trainer/sg_trainer.py:542\u001b[0m, in \u001b[0;36mTrainer._get_losses\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: torch\u001b[38;5;241m.\u001b[39mTensor, targets: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mtuple\u001b[39m]:\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# GET THE OUTPUT OF THE LOSS FUNCTION\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    544\u001b[0m         loss, loss_logging_items \u001b[38;5;241m=\u001b[39m loss\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/losses/ppyolo_loss.py:967\u001b[0m, in \u001b[0;36mPPYoloELoss.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m    964\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_batched_assignment:\n\u001b[0;32m--> 967\u001b[0m     cls_loss_sum, iou_loss_sum, dfl_loss_sum, assigned_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     cls_loss_sum, iou_loss_sum, dfl_loss_sum, assigned_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_sequential(predictions, targets)\n",
            "File \u001b[0;32m~/Projects/ukw/ukw/model/.venv/lib/python3.10/site-packages/super_gradients/training/losses/ppyolo_loss.py:791\u001b[0m, in \u001b[0;36mPPYoloELoss._forward_batched\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_batched\u001b[39m(\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    779\u001b[0m     predictions: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor],\n\u001b[1;32m    780\u001b[0m     targets: Tensor,\n\u001b[1;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor, Tensor]:\n\u001b[1;32m    782\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    Compute the loss using batched targets-anchors assignment.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m    This is the default way to compute the loss, however it may cause OOM errors when number of targets per image\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m    :return:            Tuple of (classification loss, iou loss, dfl loss, assigned scores sum)\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m     (\n\u001b[1;32m    792\u001b[0m         pred_scores,\n\u001b[1;32m    793\u001b[0m         pred_distri,\n\u001b[1;32m    794\u001b[0m         anchors,\n\u001b[1;32m    795\u001b[0m         anchor_points,\n\u001b[1;32m    796\u001b[0m         num_anchors_list,\n\u001b[1;32m    797\u001b[0m         stride_tensor,\n\u001b[1;32m    798\u001b[0m     ) \u001b[38;5;241m=\u001b[39m predictions\n\u001b[1;32m    800\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_targets_for_batched_assigner(targets, batch_size\u001b[38;5;241m=\u001b[39mpred_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# yolox -> ppyolo\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     anchor_points_s \u001b[38;5;241m=\u001b[39m anchor_points \u001b[38;5;241m/\u001b[39m stride_tensor\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 6)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing:   0%|          | 0/44 [00:02<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary module\n",
        "from super_gradients.training.losses import PPYoloELoss\n",
        "from super_gradients.training.metrics import DetectionMetrics_050\n",
        "from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback\n",
        "\n",
        "\n",
        "trainer.test(model=model,\n",
        "            test_loader=test_data,\n",
        "            test_metrics_list=DetectionMetrics_050(score_thres=0.45,\n",
        "                                                   top_k_predictions=300,\n",
        "                                                   num_cls=len(dataset_params['classes']),\n",
        "                                                   normalize_targets=True,\n",
        "                                                   post_prediction_callback=PPYoloEPostPredictionCallback(score_threshold=0.01,\n",
        "                                                                                                          nms_top_k=1000,\n",
        "                                                                                                          max_predictions=300,\n",
        "                                                                                                          nms_threshold=0.7)\n",
        "                                                  ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
