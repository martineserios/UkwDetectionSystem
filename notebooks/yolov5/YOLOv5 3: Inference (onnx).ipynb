{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "\n",
    "\n",
    "class ort_v5:\n",
    "    def __init__(self, img_path, onnx_model, conf_thres, iou_thres, img_size, classes):\n",
    "        self.img_path= img_path\n",
    "        print(self.img_path)\n",
    "        self.onnx_model=onnx_model\n",
    "        self.conf_thres=conf_thres\n",
    "        self.iou_thres =iou_thres\n",
    "        self.img_size=img_size\n",
    "        # self.cuda= cuda\n",
    "        self.names= classes\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        #image preprocessing\n",
    "        image_or= cv2.imread(self.img_path)\n",
    "        image, ratio, dwdh = self.letterbox(image_or, auto=False)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = np.expand_dims(image, 0)\n",
    "        image = np.ascontiguousarray(image)\n",
    "        im = image.astype(np.float32)\n",
    "        im /= 255\n",
    "        # print(im.shape)\n",
    "\n",
    "        #onnxruntime session\n",
    "        session= self.ort_session()\n",
    "        outname = [i.name for i in session.get_outputs()]\n",
    "        inname = [i.name for i in session.get_inputs()]\n",
    "        # print('input-output names:',inname,outname)\n",
    "        inp = {inname[0]:im}\n",
    "\n",
    "        # ONNXRuntime inference\n",
    "        t1 = time.time()\n",
    "        outputs = session.run(outname, inp)[0]\n",
    "        t2 = time.time()\n",
    "        output= torch.from_numpy(outputs)\n",
    "        out =self.non_max_suppression(output, self.conf_thres, self.iou_thres)[0]\n",
    "        print('Predictions:',out)\n",
    "        print('yolov5 ONNXRuntime Inference Time:', t2-t1)\n",
    "        img=self.result(image_or,ratio, dwdh, out)\n",
    "        cv2.imwrite('result.jpg', img)\n",
    "        # print('result', img.shape)\n",
    "        # cv2.imshow('result',img)\n",
    "        # cv2.waitKey(0)\n",
    "        return out\n",
    " \n",
    "    def box_iou(self,box1, box2, eps=1e-7):\n",
    "        # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "        \"\"\"\n",
    "        Return intersection-over-union (Jaccard index) of boxes.\n",
    "        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "        Arguments:\n",
    "            box1 (Tensor[N, 4])\n",
    "            box2 (Tensor[M, 4])\n",
    "        Returns:\n",
    "            iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "                IoU values for every element in boxes1 and boxes2\n",
    "        \"\"\"\n",
    "\n",
    "        # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "        (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n",
    "        inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "        # IoU = inter / (area1 + area2 - inter)\n",
    "        return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n",
    "\n",
    "    def non_max_suppression(self,\n",
    "            prediction,\n",
    "            conf_thres,\n",
    "            iou_thres,\n",
    "            classes=None,\n",
    "            agnostic=False,\n",
    "            multi_label=False,\n",
    "            labels=(),\n",
    "            max_det=300,\n",
    "            nm=0,  # number of masks\n",
    "    ):\n",
    "        \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "        Returns:\n",
    "            list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(prediction, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
    "            prediction = prediction[0]  # select only inference output\n",
    "\n",
    "        device = prediction.device\n",
    "        mps = 'mps' in device.type  # Apple MPS\n",
    "        if mps:  # MPS not fully supported yet, convert tensors to CPU before NMS\n",
    "            prediction = prediction.cpu()\n",
    "        bs = prediction.shape[0]  # batch size\n",
    "        nc = prediction.shape[2] - nm - 5  # number of classes\n",
    "        xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "        # Checks\n",
    "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "\n",
    "        # Settings\n",
    "        # min_wh = 2  # (pixels) minimum box width and height\n",
    "        max_wh = 7680  # (pixels) maximum box width and height\n",
    "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "        time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "        redundant = True  # require redundant detections\n",
    "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "        merge = False  # use merge-NMS\n",
    "\n",
    "        t = time.time()\n",
    "        mi = 5 + nc  # mask start index\n",
    "        output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "        for xi, x in enumerate(prediction):  # image index, image inference\n",
    "            # Apply constraints\n",
    "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "            x = x[xc[xi]]  # confidence\n",
    "\n",
    "            # Cat apriori labels if autolabelling\n",
    "            if labels and len(labels[xi]):\n",
    "                lb = labels[xi]\n",
    "                v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n",
    "                v[:, :4] = lb[:, 1:5]  # box\n",
    "                v[:, 4] = 1.0  # conf\n",
    "                v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "                x = torch.cat((x, v), 0)\n",
    "\n",
    "            # If none remain process next image\n",
    "            if not x.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Compute conf\n",
    "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "            # Box/Mask\n",
    "            box = self.xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "            mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "            # Detections matrix nx6 (xyxy, conf, cls)\n",
    "            if multi_label:\n",
    "                i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
    "                x = torch.cat((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "            else:  # best class only\n",
    "                conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "                x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "            # Filter by class\n",
    "            if classes is not None:\n",
    "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "            # Apply finite constraint\n",
    "            # if not torch.isfinite(x).all():\n",
    "            #     x = x[torch.isfinite(x).all(1)]\n",
    "\n",
    "            # Check shape\n",
    "            n = x.shape[0]  # number of boxes\n",
    "            if not n:  # no boxes\n",
    "                continue\n",
    "            elif n > max_nms:  # excess boxes\n",
    "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "            else:\n",
    "                x = x[x[:, 4].argsort(descending=True)]  # sort by confidence\n",
    "\n",
    "            # Batched NMS\n",
    "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "            if i.shape[0] > max_det:  # limit detections\n",
    "                i = i[:max_det]\n",
    "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "                iou = self.box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "                weights = iou * scores[None]  # box weights\n",
    "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "                if redundant:\n",
    "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "            output[xi] = x[i]\n",
    "            if mps:\n",
    "                output[xi] = output[xi].to(device)\n",
    "            if (time.time() - t) > time_limit:\n",
    "                # LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "                break  # time limit exceeded\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def xywh2xyxy(self, x):\n",
    "        # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "        y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "        y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "        y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "        y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "        y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "        return y\n",
    "\n",
    "    # Read classes.txt \n",
    "    def class_name(self):\n",
    "        classes=[]\n",
    "        file= open(self.names,'r')\n",
    "        while True:\n",
    "          name=file.readline().strip('\\n')\n",
    "          classes.append(name)\n",
    "          if not name:\n",
    "            break\n",
    "        return classes\n",
    "\n",
    "    def letterbox(self, im, color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "        # Resize and pad image while meeting stride-multiple constraints\n",
    "        shape = im.shape[:2]  # current shape [height, width]\n",
    "        new_shape= self.img_size\n",
    "        if isinstance(new_shape, int):\n",
    "            new_shape = (new_shape, new_shape)\n",
    "\n",
    "        # Scale ratio (new / old)\n",
    "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "        if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "            r = min(r, 1.0)\n",
    "\n",
    "        # Compute padding\n",
    "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "        if auto:  # minimum rectangle\n",
    "            dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "        dw /= 2  # divide padding into 2 sides\n",
    "        dh /= 2\n",
    "\n",
    "        if shape[::-1] != new_unpad:  # resize\n",
    "            im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "        im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "        return im, r, (dw, dh)\n",
    "\n",
    "    # Initialize ONNXRuntime session   \n",
    "    def ort_session(self):\n",
    "        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if ort.get_device()=='GPU' else ['CPUExecutionProvider']\n",
    "        session = ort.InferenceSession(self.onnx_model, providers=providers)\n",
    "        print(session.get_providers())\n",
    "        return session\n",
    "\n",
    "    # Display results\n",
    "    def result(self,img,ratio, dwdh, out):\n",
    "        names= self.class_name()\n",
    "        colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}   \n",
    "        for i,(x0,y0,x1,y1,score,cls_id) in enumerate(out):\n",
    "            box = np.array([x0,y0,x1,y1])\n",
    "            box -= np.array(dwdh*2)\n",
    "            box /= ratio\n",
    "            box = box.round().astype(np.int32).tolist()\n",
    "            cls_id = int(cls_id)\n",
    "            score = round(float(score),3)\n",
    "            name = names[cls_id]\n",
    "            color = colors[name]\n",
    "            name += ' '+str(score)\n",
    "            cv2.rectangle(img,box[:2],box[2:],color,2)\n",
    "            cv2.putText(img,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2) \n",
    "        return img\n",
    "        # cv2.imwrite('v5_onnx.jpg', img) \n",
    "        # cv2.imshow('result',img)\n",
    "        # cv2.waitKey(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classes.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile classes.txt\n",
    "smoke\n",
    "fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/martin/Projects/ongoing/ukw/data/test_images/original/test_def.jpeg\n",
      "['CPUExecutionProvider']\n",
      "Predictions: tensor([], size=(0, 6))\n",
      "yolov5 ONNXRuntime Inference Time: 0.23824644088745117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 6))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMAGE_LOCATION = '/home/martin/Projects/ongoing/ukw/data/test_images/original/test_def.jpeg'\n",
    "ONNX_PRETRAINED_MODEL_LOCATION = '/home/martin/Projects/ongoing/ukw/ukw/wildfire-detector/model/yolov5_freeze_ds_fire_3_pre/best.onnx'\n",
    "conf_thres = 0.1\n",
    "iou_thresh = 0.5\n",
    "imgs = (640.640)\n",
    "classes = 'classes.txt'\n",
    "\n",
    "\n",
    "ORT= ort_v5(TEST_IMAGE_LOCATION, ONNX_PRETRAINED_MODEL_LOCATION, conf_thres=conf_thres, iou_thres=iou_thresh, img_size=(640,640), classes=classes)\n",
    "ORT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
